{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easyPCA: PCA and cluster analysis\n",
    "Principal component analysis (PCA) with optional classification and HDBSCAN clustering in the PCA score plot.\n",
    "\n",
    "Jupyter Notebook is provided in https://github.com/danield5732/easyPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todos\n",
    "\n",
    "very nice to have, but still nice to have:\n",
    "- plot PCA Score graph with ploty to enable hoover option\n",
    "- include loadings graph for selected PCs to Score plot block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Lucas: Dat1_pretreatment_v7 and Michael Galarnyk: PCA_Data_Visualization_Iris_Dataset_Blog\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import scipy.signal\n",
    "import os\n",
    "import sklearn.preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objs as go\n",
    "import plotly as py\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "#import umap   # https://umap-learn.readthedocs.io/en/latest/\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DataFrame from Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load df from Excel\n",
    "# dtype is not interpreted and therefore, preserve accuracy\n",
    "df = pd.read_excel('PCA-FSA_03.xlsx', index_col = 0, header = 0, dtype = 'object')   \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature and classifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['HK 950', 'HKP 1050', 'CCP 90D', '5000 P-f', 'AZ 1050', 'PS WP 235', 'CC 401']\n",
    "classifiers = ['analytics', 'category']\n",
    "\n",
    "#features = df.drop(classifiers, axis = 1).columns\n",
    "#classifiers = df.drop(features, axis = 1).columns\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preselect sample sets from loaded DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter for certain criteria or analyze the whole DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_select = df.copy()\n",
    "\n",
    "#mask = ((df.loc[:,'source'].values == 'Pia_PROTECT') & \n",
    "#        (df.loc[:,'washed'].values != 'washed'))\n",
    "\n",
    "#mask = (df.loc[:,'washed'].values != 'washed')\n",
    "\n",
    "df_select = df.loc[mask,(list(features) + list(classifiers))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle non numeric values in feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set strings in feature columns to NaN\n",
    "for feature in features:\n",
    "    df_select.loc[:,feature] = pd.to_numeric(df_select.loc[:,feature], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle NaN values in classifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NaN values in classifier columns to unassigned\n",
    "df_select.loc[:,classifiers] = df_select.loc[:,classifiers].fillna('unassigned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle NaN values in feature columns (Think about standardization and normalization in advance, see below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude samples with NaN in feature values\n",
    "df_select = df_select.dropna(subset = features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# set NaN in feature values to 0\n",
    "df_select.loc[:,features] = df_select.loc[:,features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization and normaliazion of the data: What are you looking for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways for data pretreatment. \n",
    "This section will lead you through the common options (at least for me).\n",
    "\n",
    "To achieve what you are actual looking for, it is necessary to think about it in advance.\n",
    "Otherwise, you will get a result anyway, but it is even harder to interprete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature data from selected DataFrame\n",
    "x = df_select.loc[:, features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Samples\n",
    "In general, features are located in columns and samples in rows.\n",
    "If you want to swap this, you have to transpose your input data first.\n",
    "\n",
    "#### When is it usefull to treat your samples as features?\n",
    "- When you are interested in your samples, you should leave them as rows and you will see samples with similar features clustered in the scores plot.\n",
    "- When you want to switch the perspective to see how your chosen features are distributed through your samples, you could think about the previous transponation.\n",
    "\n",
    "\n",
    "- This directly effects the number of dots you will find in the PCA Scores plot, that originate from the number of rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize or normalize column-wise (features)\n",
    "Performing a standardization or normalization along your features, will differentiate your samples along each feature.\n",
    "Meaning the sample with the lowest feature value will stay the smales and the biggest will stay the biggest, but they are scaled into a range of e.g. -1 to 1.\n",
    "Since this is done for every feature, the features will contribute to the PCA result equally.\n",
    "Otherwise, features with larger scales would be overrepresented and may suppress the impact of other features.\n",
    "\n",
    "**This is usefill,**\n",
    "- if your features are independent of each other or having different units.\n",
    "- if you want to see the effects of minor properties represented by a feature.\n",
    "\n",
    "**This may not be usefull,**\n",
    "- if your features are already similar and their values are directly comparable to each other.\n",
    "- if your features with small values have high uncertainties that will be scaled up, too and therefore, impair the PCA result.\n",
    "- if your feature values are related to sample properties that are nor represented by any feature, e.g. sample weight. Since, then the values are not comparable directly.\n",
    "- if you have NaN values in your features. These rows need to be exluded or to be set to a fixed value. The latter would effect the PCA result by an unjustified manner, for my point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization vs. normalization\n",
    "**Standardization** only can be done column-wise, since it centers the data by shifting the mean value to 0.\n",
    "Further, it squeezes the data by setting the standard deviation to -1 and 1.\n",
    "Therefore, your data is assumed to be distributed normaly according to Gauss and the distance between each value is no longer relative the same as in the original data.\n",
    "If the assumption is true, your data point in the PCA results may be better clustered than by normalization.\n",
    "\n",
    "On the other hand, **normalization** keeps the relative distances between your data by only scaling them to certain range. To center the data is recommended anyway.\n",
    "There are different approaches for normalization:\n",
    "- norm = 'max': the lowest value is set to 0 and the highest value is set to 1, therefore the maximum variance within the data is obtained. On the contraty, information about the minimum value is lost, if it was already 0 or only the smalles value of the feature.\n",
    "- *There should be an max-norm respecting 0 as 0 and only scale by the maximum value.*\n",
    "- norm = 'l1': I have no idea what is going on.\n",
    "- norm = 'l2': I have no idea what is going on.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# standardize each feature colum, includes centering to the mean\n",
    "# transformation of the data onto unit scale (mean=0 and variance=1)\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and center the data. Options for norm are 'l1', 'l2' or 'max'\n",
    "# 'max' is stretching the values beeing the smallest 0\n",
    "x = normalize(x, norm='max', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize row-wise (samples)\n",
    "Normalization of a sample's features will create a sample specific pattern.\n",
    "Standardization is not available row-wise.\n",
    "\n",
    "**This is usefill,**\n",
    "- if your features are comprised of absolute values not respecting specific differences within the samples, e.g. sample weight. This will rule out diffences in the samples that may occure independent of its features and compare the samples specific feature pattern.\n",
    "\n",
    "**This may not be usefull,**\n",
    "- if you want to see differences within on feature in several samples, since this relation gets totally lost."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# normalize and center the data. Options for norm are 'l1', 'l2' or 'max'\n",
    "# 'max' is stretching the values beeing the smallest 0\n",
    "x = normalize(x, norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centering of the data is done anyway by the method used in the PCA class from the decomposition module in the sklearn (scikit-learn) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data ready to be used in PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data = x, columns = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and clustering of the selected and pretreated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PC_labels(pca_explvar):\n",
    "    names = ['PC']*len(pca_explvar)\n",
    "    for counter, value in enumerate(names):\n",
    "        names[counter] = value+str(counter+1)\n",
    "    return names\n",
    "\n",
    "#t = spec_sg_norm[spec_sg_norm['thresh_0.06']==True].iloc[:,10:len(xaxis)-10]\n",
    "#t = np.array(t, dtype=np.float32)\n",
    "t = x\n",
    "\n",
    "if(min(len(features), len(df_select))>9):\n",
    "    n_components = 9\n",
    "else:\n",
    "    n_components = min(len(features), len(df_select)) -1\n",
    "\n",
    "pca          = PCA(n_components = n_components)\n",
    "pca_scores   = pca.fit_transform(t)\n",
    "pca_loadings = pca.components_\n",
    "pca_explvar  = pca.explained_variance_ratio_\n",
    "\n",
    "pca_scores   = pd.DataFrame(pca_scores)\n",
    "pca_scores.columns  = PC_labels(pca_explvar)\n",
    "legend        = PC_labels(pca_explvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n",
    "                            gen_min_span_tree=False, leaf_size=40,\n",
    "                            metric='euclidean', min_cluster_size=3, min_samples=2, p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#legend = PC_labels(pca_explvar)\n",
    "#d      = {}\n",
    "#for index, value in enumerate(pca_scores.columns.values[:-1]):\n",
    "#    d[value] = index\n",
    "    \n",
    "# the widgets\n",
    "PC_x = widgets.Dropdown(options=pca_scores.columns, value=pca_scores.columns[0], description='PC_x: ', disabled=False,)\n",
    "PC_y = widgets.Dropdown(options=pca_scores.columns, value=pca_scores.columns[1], description='PC_y: ', disabled=False,)\n",
    "PC_class = widgets.Dropdown(options = ['None'] + list(classifiers) + ['Cluster: HDBSCAN'], value='None', description='classify: ', disabled=False,)\n",
    "\n",
    "\n",
    "def update_scores(PC_x, PC_y, PC_class):\n",
    "    #PCx = d[PC_x]\n",
    "    #PCy = d[PC_y]\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1,figsize=(8,8))\n",
    "    \n",
    "    if (PC_class == 'Cluster: HDBSCAN'):\n",
    "        s = np.array(pca_scores[[PC_x, PC_y]])   # get current scores\n",
    "        hdb = clusterer.fit(s)                   # perform clustering\n",
    "        hdbscan = clusterer.labels_              # get cluster results\n",
    "        hdbscan = hdbscan+1                      # shift values, beeing unassignes -1: 0 \n",
    "        hdbscan = ['Cluster ' + s for s in hdbscan.astype(str)]   # set values to strings and modify\n",
    "        hdbscan = np.array(hdbscan)              # retransform list to np.array\n",
    "        hdbscan = np.where(hdbscan == 'Cluster 0', 'unassigned', hdbscan)   # rename Cluster 0 to 'unassigned'\n",
    "        classification = hdbscan\n",
    "        df_select.loc[:,'HDBSCAN: '+PC_x+'-'+PC_y] = hdbscan\n",
    "    elif (PC_class == 'None'):\n",
    "        classification = ['PCA Scores']\n",
    "    else:\n",
    "        classification = df_select[PC_class].values\n",
    "    \n",
    "    targets = sorted(list(set(classification)))\n",
    "    for target in targets:\n",
    "        if (PC_class == 'None'):\n",
    "            indicesToKeep = pca_scores.index\n",
    "        else:\n",
    "            indicesToKeep = classification == target\n",
    "                \n",
    "        ax.scatter(pca_scores.loc[indicesToKeep,PC_x],\n",
    "                    pca_scores.loc[indicesToKeep,PC_y],\n",
    "                    s = 75)\n",
    "\n",
    "    ax = fig.gca()\n",
    "    #ax.grid()\n",
    "    ax.hlines(y=0, xmin = min(pca_scores.loc[:,PC_x]), xmax = max(pca_scores.loc[:,PC_x]), color='g', linestyle='dotted')\n",
    "    ax.vlines(x=0, ymin = min(pca_scores.loc[:,PC_y]), ymax = max(pca_scores.loc[:,PC_y]), color='g', linestyle='dotted')\n",
    "    ax.set_xlabel(PC_x, fontsize = 15)\n",
    "    ax.set_ylabel(PC_y, fontsize = 15)\n",
    "    ax.set_title('PCA Score plot', fontsize = 20)\n",
    "    \n",
    "    ax.legend(targets)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "legend = PC_labels(pca_explvar)\n",
    "# the widgets\n",
    "loadings = widgets.SelectMultiple(options = np.array(range(len(pca_explvar)), dtype=int), value = (0, ),\n",
    "                                  description = 'Loadings: ')\n",
    "\n",
    "def update_loadings(loadings):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax2 = plt.subplot2grid((2, 1), (0,0), colspan=2)\n",
    "    ax3 = plt.subplot2grid((2, 1), (1,0), colspan=2)\n",
    "\n",
    "    # plot the loadings\n",
    "    leg = [legend[i] for i in loadings]\n",
    "    ax2.plot(features,pca_loadings[loadings,:].T)\n",
    "    #ax2.invert_xaxis()\n",
    "    ax2.hlines(y=0, xmin = 0, xmax = len(pca_explvar), color='g', linestyle='dotted')\n",
    "    #ax2.legend(leg, loc='center left', bbox_to_anchor=(1, 0.5) )\n",
    "    ax2.set_ylabel('')\n",
    "    ax2.set_title('Loadings of ' + str(leg))\n",
    "    \n",
    "    # plot the explained variance\n",
    "    sns.lineplot(x=list(range(1,len(pca_explvar)+1)), y=pca_explvar, ax=ax3)\n",
    "    sns.lineplot(x=list(range(1,len(pca_explvar)+1)), y=np.cumsum(pca_explvar), ax=ax3)\n",
    "    ax3.legend(['PC_explvar','sum curve'])#, loc='center left', bbox_to_anchor=(1, 0.5) )\n",
    "    ax3.set_xlabel('No. principal component')\n",
    "    ax3.set_ylabel('Explained variance')\n",
    "    ax3.set_title('Explained variance')\n",
    "    ax3.set_xticks(np.linspace(1,len(pca_explvar),len(pca_explvar), dtype=np.int))\n",
    "    ax3.set_xticklabels(np.linspace(1,len(pca_explvar),len(pca_explvar), dtype=np.int))\n",
    "    ax3.set_ylim = [0,1]\n",
    "    ax3.yaxis.grid(True)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(update_scores, PC_x=PC_x, PC_y=PC_y, PC_class=PC_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(update_loadings, loadings=loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data used for PCA\n",
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data of the principal components\n",
    "pca_scores_save = pca_scores.copy()\n",
    "pca_scores_save.set_index(df_select.index, inplace = True)\n",
    "pca_scores_save = pd.concat([pca_scores_save, df_select.loc[:,list(classifiers)]], axis=1)\n",
    "pca_scores_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save PCA results to Excel\n",
    "with pd.ExcelWriter('PCA_results.xlsx') as writer:\n",
    "    df_select.to_excel(writer, sheet_name = 'df_select')\n",
    "    pca_scores_save.to_excel(writer, sheet_name = 'pca_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
